{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd7ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Input # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793756",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() # 학습셋과 평가셋 분리해서 로드\n",
    "is_normalized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d351f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_normalized: # is_normalized 변수가 False라면\n",
    "  train_images = train_images / 255.0 # 0~255 to 0~1\n",
    "  test_images = test_images / 255.0 # 0~255 to 0~1\n",
    "\n",
    "  is_normalized = True # 정규화 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745baff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8324 - loss: 0.4647 - val_accuracy: 0.8519 - val_loss: 0.4214 - learning_rate: 0.0010\n",
      "Epoch 2/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.8641 - loss: 0.3673 - val_accuracy: 0.8476 - val_loss: 0.4242 - learning_rate: 0.0010\n",
      "Epoch 3/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.8735 - loss: 0.3427 - val_accuracy: 0.8737 - val_loss: 0.3493 - learning_rate: 0.0010\n",
      "Epoch 4/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - accuracy: 0.8787 - loss: 0.3256 - val_accuracy: 0.8751 - val_loss: 0.3558 - learning_rate: 0.0010\n",
      "Epoch 5/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 19ms/step - accuracy: 0.8837 - loss: 0.3110 - val_accuracy: 0.8795 - val_loss: 0.3386 - learning_rate: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.8908 - loss: 0.2926 - val_accuracy: 0.8703 - val_loss: 0.3660 - learning_rate: 0.0010\n",
      "Epoch 7/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 20ms/step - accuracy: 0.8946 - loss: 0.2816 - val_accuracy: 0.8784 - val_loss: 0.3396 - learning_rate: 0.0010\n",
      "Epoch 8/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.8979 - loss: 0.2718 - val_accuracy: 0.8611 - val_loss: 0.3839 - learning_rate: 0.0010\n",
      "Epoch 9/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9016 - loss: 0.2629 - val_accuracy: 0.8813 - val_loss: 0.3275 - learning_rate: 0.0010\n",
      "Epoch 10/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 20ms/step - accuracy: 0.9049 - loss: 0.2544 - val_accuracy: 0.8737 - val_loss: 0.3502 - learning_rate: 0.0010\n",
      "Epoch 11/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.9090 - loss: 0.2430 - val_accuracy: 0.8855 - val_loss: 0.3259 - learning_rate: 0.0010\n",
      "Epoch 12/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9104 - loss: 0.2370 - val_accuracy: 0.8811 - val_loss: 0.3361 - learning_rate: 0.0010\n",
      "Epoch 13/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9119 - loss: 0.2325 - val_accuracy: 0.8636 - val_loss: 0.4000 - learning_rate: 0.0010\n",
      "Epoch 14/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.9148 - loss: 0.2254 - val_accuracy: 0.8927 - val_loss: 0.3031 - learning_rate: 0.0010\n",
      "Epoch 15/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9158 - loss: 0.2212 - val_accuracy: 0.8865 - val_loss: 0.3328 - learning_rate: 0.0010\n",
      "Epoch 16/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9192 - loss: 0.2138 - val_accuracy: 0.8878 - val_loss: 0.3145 - learning_rate: 0.0010\n",
      "Epoch 17/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9191 - loss: 0.2131 - val_accuracy: 0.8914 - val_loss: 0.3181 - learning_rate: 0.0010\n",
      "Epoch 18/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 18ms/step - accuracy: 0.9214 - loss: 0.2076 - val_accuracy: 0.8967 - val_loss: 0.3120 - learning_rate: 0.0010\n",
      "Epoch 19/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.9357 - loss: 0.1691 - val_accuracy: 0.9030 - val_loss: 0.2875 - learning_rate: 3.0000e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.9405 - loss: 0.1562 - val_accuracy: 0.9017 - val_loss: 0.2988 - learning_rate: 3.0000e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 17ms/step - accuracy: 0.9427 - loss: 0.1516 - val_accuracy: 0.9034 - val_loss: 0.2963 - learning_rate: 3.0000e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9457 - loss: 0.1446 - val_accuracy: 0.9037 - val_loss: 0.2992 - learning_rate: 3.0000e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9463 - loss: 0.1400 - val_accuracy: 0.9015 - val_loss: 0.3109 - learning_rate: 3.0000e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9524 - loss: 0.1264 - val_accuracy: 0.9065 - val_loss: 0.2989 - learning_rate: 9.0000e-05\n",
      "Epoch 25/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9523 - loss: 0.1238 - val_accuracy: 0.9047 - val_loss: 0.3021 - learning_rate: 9.0000e-05\n",
      "Epoch 26/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9533 - loss: 0.1217 - val_accuracy: 0.9054 - val_loss: 0.3047 - learning_rate: 9.0000e-05\n",
      "Epoch 27/40\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 16ms/step - accuracy: 0.9543 - loss: 0.1183 - val_accuracy: 0.9050 - val_loss: 0.3116 - learning_rate: 9.0000e-05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# copy from https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "\n",
    "  def on_train_begin(self, logs={}): # 훈련이 시작될 때 케라스가 자동으로 호출해주는 핵심 구간\n",
    "\n",
    "    self.i = 0\n",
    "    self.x = []\n",
    "    self.losses = []\n",
    "    self.val_losses = []\n",
    "\n",
    "    self.fig = plt.figure()\n",
    "\n",
    "    self.logs = []\n",
    "\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs={}): # 매 에포크가 끝날 때마다 케라스가 자동으로 호출해주는 핵심 구간\n",
    "\n",
    "    self.logs.append(logs)\n",
    "    self.x.append(self.i)\n",
    "    self.losses.append(logs.get('loss'))\n",
    "    self.val_losses.append(logs.get('val_loss'))\n",
    "    self.i += 1\n",
    "\n",
    "    clear_output(wait=True) # 이전 에포크에서 그렸던 그래프를 지워줍니다.\n",
    "    plt.plot(self.x, self.losses, label=\"loss\")\n",
    "    plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"loss = \", self.losses[-1], \", val_loss = \", self.val_losses[-1])\n",
    "\n",
    "\n",
    "\n",
    "# 입력층\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "\n",
    "# 은닉층 (Dense 기반)\n",
    "x = keras.layers.Dense(1024, activation='gelu', kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.15)(x)\n",
    "\n",
    "x = keras.layers.Dense(512, activation='gelu', kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.10)(x)\n",
    "\n",
    "x = keras.layers.Dense(256, activation='gelu', kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Dropout(0.05)(x)\n",
    "\n",
    "x = keras.layers.Dense(128, activation='gelu', kernel_initializer='he_normal')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# 출력층 (분류 문제 → Softmax)\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# 모델 정의\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# compile도 한 번만\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# -------------------------\n",
    "# 콜백 정의\n",
    "# -------------------------\n",
    "\n",
    "model_check_point = ModelCheckpoint('best_model.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "plot_losses = PlotLosses()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_learning_rate=0.001)\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        patience=8, restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.3,\n",
    "        patience=4,\n",
    "        min_lr=1e-5\n",
    "    )\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 학습 (Functional DNN 모델에 맞게 수정)\n",
    "# -------------------------\n",
    "history = model.fit(\n",
    "    train_images, \n",
    "    keras.utils.to_categorical(train_labels, 10),   # 원-핫 인코딩\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(\n",
    "        test_images, \n",
    "        keras.utils.to_categorical(test_labels, 10) # 원-핫 인코딩\n",
    "    ),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
